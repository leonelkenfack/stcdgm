{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåç ST-CDGM Training & Evaluation Notebook\n",
        "## Spatio-Temporal Causal Diffusion Generative Model\n",
        "\n",
        "Ce notebook vous permet de :\n",
        "1. **Explorer les donn√©es** climatiques NetCDF\n",
        "2. **Entra√Æner le mod√®le** ST-CDGM\n",
        "3. **Tester et √©valuer** les performances\n",
        "4. **Visualiser les r√©sultats** de downscaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Motivation et Id√©e Centrale\n",
        "\n",
        "### üí° **Motivation**\n",
        "Remplacer le bruit statistique al√©atoire par un **signal physique** g√©n√©r√© causalement\n",
        "\n",
        "### üî¨ **Id√©e**\n",
        "Utiliser la causalit√© pour g√©n√©rer les **d√©tails fins** (incertitudes)\n",
        "\n",
        "---\n",
        "\n",
        "**Concept cl√© du ST-CDGM** : Au lieu d'utiliser du bruit gaussien purement al√©atoire comme les mod√®les de diffusion classiques, le ST-CDGM g√©n√®re les d√©tails haute r√©solution en exploitant les relations causales entre les variables climatiques. Cela permet de produire des pr√©dictions physiquement coh√©rentes et r√©alistes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Installation et Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:17:59.760719Z",
          "start_time": "2025-11-25T23:17:59.702018Z"
        }
      },
      "outputs": [],
      "source": [
        "# Installation des d√©pendances (d√©commenter si n√©cessaire)\n",
        "# !pip install torch xarray pandas numpy matplotlib seaborn xbatcher netcdf4 h5netcdf hydra-core\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:18:20.838508Z",
          "start_time": "2025-11-25T23:17:59.840277Z"
        }
      },
      "outputs": [],
      "source": [
        "# Configuration compl√®te pour ex√©cution locale\n",
        "# Auto-reload des modules locaux (utile si vous modifiez le code dans src/ pendant la session)\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# D√©finir le r√©pertoire de travail\n",
        "project_root = Path.cwd()\n",
        "os.chdir(project_root)\n",
        "\n",
        "# Ajouter src au PYTHONPATH\n",
        "if str(project_root / \"src\") not in sys.path:\n",
        "    sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Imports scientifiques\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors  # Pour CenteredNorm (centrage de colormap)\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Imports depuis le package st_cdgm\n",
        "from st_cdgm import (\n",
        "    NetCDFDataPipeline,\n",
        "    HeteroGraphBuilder,\n",
        "    IntelligibleVariableEncoder,\n",
        "    RCNCell,\n",
        "    RCNSequenceRunner,\n",
        "    CausalDiffusionDecoder,\n",
        "    train_epoch,\n",
        ")\n",
        "\n",
        "# Pour IntelligibleVariableConfig (si n√©cessaire)\n",
        "from st_cdgm.models.intelligible_encoder import IntelligibleVariableConfig\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# V√©rifications\n",
        "print(f\"‚úÖ R√©pertoire de travail: {os.getcwd()}\")\n",
        "print(f\"‚úÖ PYTHONPATH configur√©: {str(project_root / 'src') in sys.path}\")\n",
        "print(f\"‚úÖ Package st_cdgm disponible: {Path('src/st_cdgm').exists()}\")\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"‚úÖ Tous les imports r√©ussis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÇÔ∏è 2. Exploration des Donn√©es\n",
        "\n",
        "### 2.1 Configuration des chemins\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:18:20.905475Z",
          "start_time": "2025-11-25T23:18:20.896189Z"
        }
      },
      "outputs": [],
      "source": [
        "# Chemins relatifs depuis le r√©pertoire du projet\n",
        "# Structure data/ : raw/train/ (entra√Ænement), raw/normalization_coefs/, raw/static_predictors/, raw/*.nc (test/eval)\n",
        "from pathlib import Path\n",
        "DATA_ROOT = Path(\"data/raw\")\n",
        "LR_PATH = str(DATA_ROOT / \"train\" / \"predictor_ACCESS-CM2_hist.nc\")\n",
        "HR_PATH = str(DATA_ROOT / \"train\" / \"pr_ACCESS-CM2_hist.nc\")\n",
        "STATIC_PATH = str(DATA_ROOT / \"static_predictors\" / \"ERA5_eval_ccam_12km.198110_NZ_Invariant.nc\")\n",
        "MEAN_PATH = str(DATA_ROOT / \"normalization_coefs\" / \"mean_1974_2011.nc\")\n",
        "STD_PATH = str(DATA_ROOT / \"normalization_coefs\" / \"std_1974_2011.nc\")\n",
        "\n",
        "# V√©rifier l'existence des fichiers\n",
        "from pathlib import Path\n",
        "for path, name in [(LR_PATH, \"LR\"), (HR_PATH, \"HR\")]:\n",
        "    if path and Path(path).exists():\n",
        "        print(f\"‚úÖ {name} dataset trouv√©: {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name} dataset manquant: {path}\")\n",
        "\n",
        "# V√©rifier STATIC_PATH, MEAN_PATH, STD_PATH\n",
        "for path, name in [(STATIC_PATH, \"Static\"), (MEAN_PATH, \"Mean\"), (STD_PATH, \"Std\")]:\n",
        "    if path and Path(path).exists():\n",
        "        print(f\"‚úÖ {name} trouv√©: {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name} manquant: {path}\")\n",
        "if not Path(STATIC_PATH).exists():\n",
        "    STATIC_PATH = None\n",
        "if not Path(MEAN_PATH).exists():\n",
        "    MEAN_PATH = None\n",
        "if not Path(STD_PATH).exists():\n",
        "    STD_PATH = None\n",
        "\n",
        "# Afficher la structure data/raw/\n",
        "data_dir = Path(\"data/raw\")\n",
        "if data_dir.exists():\n",
        "    print(f\"\\nüìÅ Structure data/raw/:\")\n",
        "    for sub in sorted(data_dir.iterdir()):\n",
        "        if sub.is_dir():\n",
        "            files = list(sub.glob(\"*.nc\")) or list(sub.glob(\"*.json\")) or []\n",
        "            print(f\"   {sub.name}/: {[f.name for f in files[:5]]}{'...' if len(files) > 5 else ''}\")\n",
        "        else:\n",
        "            print(f\"   {sub.name}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  R√©pertoire data/raw/ n'existe pas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Chargement et inspection des donn√©es brutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:18:21.654226Z",
          "start_time": "2025-11-25T23:18:20.959712Z"
        }
      },
      "outputs": [],
      "source": [
        "# Charger les datasets bruts\n",
        "lr_raw = xr.open_dataset(LR_PATH)\n",
        "hr_raw = xr.open_dataset(HR_PATH)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä LOW RESOLUTION DATASET\")\n",
        "print(\"=\" * 60)\n",
        "print(lr_raw)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä HIGH RESOLUTION DATASET\")\n",
        "print(\"=\" * 60)\n",
        "print(hr_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:18:21.724227Z",
          "start_time": "2025-11-25T23:18:21.683070Z"
        }
      },
      "outputs": [],
      "source": [
        "# Informations sur les dimensions\n",
        "def print_dataset_info(ds, name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üìê {name} DATASET INFO\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Variables: {list(ds.data_vars)}\")\n",
        "    print(f\"Dimensions: {dict(ds.dims)}\")\n",
        "    print(f\"Coordinates: {list(ds.coords)}\")\n",
        "    if 'time' in ds.dims or 'time' in ds.coords:\n",
        "        time_coord = 'time' if 'time' in ds.coords else list(ds.coords)[0]\n",
        "        try:\n",
        "            # G√©rer diff√©rents types de coordonn√©es temporelles\n",
        "            time_values = ds[time_coord].values\n",
        "            if hasattr(time_values[0], 'strftime'):  # datetime object\n",
        "                start_time = time_values[0]\n",
        "                end_time = time_values[-1]\n",
        "            else:  # string ou autre format\n",
        "                start_time = pd.to_datetime(str(time_values[0]))\n",
        "                end_time = pd.to_datetime(str(time_values[-1]))\n",
        "            print(f\"P√©riode: {start_time} ‚Üí {end_time}\")\n",
        "            print(f\"Nombre de pas de temps: {len(ds[time_coord])}\")\n",
        "        except Exception as e:\n",
        "            # En cas d'erreur, afficher les valeurs brutes\n",
        "            print(f\"Coordonn√©es temporelles: {ds[time_coord].values[0]} ‚Üí {ds[time_coord].values[-1]}\")\n",
        "            print(f\"Nombre de pas de temps: {len(ds[time_coord])}\")\n",
        "\n",
        "print_dataset_info(lr_raw, \"LOW RESOLUTION\")\n",
        "print_dataset_info(hr_raw, \"HIGH RESOLUTION\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Visualisation des donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:18:23.280061Z",
          "start_time": "2025-11-25T23:18:21.765551Z"
        }
      },
      "outputs": [],
      "source": [
        "# Visualiser un snapshot des donn√©es LR et HR\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# LR data\n",
        "lr_var = list(lr_raw.data_vars)[0]\n",
        "lr_raw[lr_var].isel(time=0).plot(ax=axes[0], cmap='viridis')\n",
        "axes[0].set_title(f'Low Resolution - {lr_var} (t=0)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# HR data\n",
        "hr_var = list(hr_raw.data_vars)[0]\n",
        "hr_raw[hr_var].isel(time=0).plot(ax=axes[1], cmap='RdBu_r')\n",
        "axes[1].set_title(f'High Resolution - {hr_var} (t=0)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Cr√©ation du pipeline de donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:27.160745Z",
          "start_time": "2025-11-25T23:18:23.297036Z"
        }
      },
      "outputs": [],
      "source": [
        "# Configuration du pipeline (align√©e sur downscaling: 8 canaux LR, pr en HR)\n",
        "SEQ_LEN = 6  # Longueur des s√©quences temporelles\n",
        "BASELINE_STRATEGY = \"hr_smoothing\"  # ou \"lr_interp\"\n",
        "BASELINE_FACTOR = 2  # plus petit => plus l√©ger\n",
        "NORMALIZE = True  # Normalisation LR avec mean/std (comme downscaling)\n",
        "PRECIPITATION_DELTA = 0.01  # Delta pour log1p (config downscaling: delta=0.01)\n",
        "NAN_FILL_STRATEGY = \"zero\"  # Strat√©gie de nettoyage NaN: \"zero\", \"mean\", ou \"interpolate\"\n",
        "\n",
        "# Variables comme downscaling (var_names + cible pr)\n",
        "LR_VARIABLES = [\"q_500\", \"q_850\", \"u_500\", \"u_850\", \"v_500\", \"v_850\", \"t_500\", \"t_850\"]  # 8 canaux LR\n",
        "HR_VARIABLES = [\"pr\"]  # Cible pr√©cipitations (comme train_y dans downscaling)\n",
        "STATIC_VARIABLES = [\"orog\", \"he\", \"vegt\"] if STATIC_PATH else None  # Champs statiques downscaling\n",
        "\n",
        "# S√©curit√©: afficher les chemins r√©ellement utilis√©s\n",
        "print(f\"LR_PATH actif: {LR_PATH}\")\n",
        "print(f\"HR_PATH actif: {HR_PATH}\")\n",
        "print(f\"STATIC_PATH: {STATIC_PATH}\")\n",
        "print(f\"MEAN_PATH: {MEAN_PATH}, STD_PATH: {STD_PATH}\")\n",
        "\n",
        "# Valider que les variables existent, sinon fallback automatique\n",
        "import xarray as xr\n",
        "lr_ds_preview = xr.open_dataset(LR_PATH)\n",
        "hr_ds_preview = xr.open_dataset(HR_PATH)\n",
        "\n",
        "lr_avail = set(lr_ds_preview.data_vars)\n",
        "hr_avail = set(hr_ds_preview.data_vars)\n",
        "\n",
        "if not set(LR_VARIABLES).issubset(lr_avail):\n",
        "    print(f\"‚ö†Ô∏è  LR_VARIABLES invalides: {set(LR_VARIABLES) - lr_avail}\")\n",
        "    LR_VARIABLES = [v for v in LR_VARIABLES if v in lr_avail] or sorted(lr_avail)[:8]\n",
        "    print(f\"‚û°Ô∏è  Fallback LR_VARIABLES = {LR_VARIABLES}\")\n",
        "\n",
        "if not set(HR_VARIABLES).issubset(hr_avail):\n",
        "    print(f\"‚ö†Ô∏è  HR_VARIABLES invalides: {set(HR_VARIABLES) - hr_avail}\")\n",
        "    HR_VARIABLES = [sorted(hr_avail)[0]]\n",
        "    print(f\"‚û°Ô∏è  Fallback HR_VARIABLES = {HR_VARIABLES}\")\n",
        "\n",
        "print(\"üîÑ Cr√©ation du pipeline de donn√©es...\")\n",
        "pipeline = NetCDFDataPipeline(\n",
        "    lr_path=LR_PATH,\n",
        "    hr_path=HR_PATH,\n",
        "    static_path=STATIC_PATH,\n",
        "    seq_len=SEQ_LEN,\n",
        "    baseline_strategy=BASELINE_STRATEGY,\n",
        "    baseline_factor=BASELINE_FACTOR,\n",
        "    normalize=NORMALIZE,\n",
        "    nan_fill_strategy=NAN_FILL_STRATEGY,\n",
        "    precipitation_delta=PRECIPITATION_DELTA,\n",
        "    lr_variables=LR_VARIABLES,\n",
        "    hr_variables=HR_VARIABLES,\n",
        "    static_variables=STATIC_VARIABLES,\n",
        "    means_path=MEAN_PATH if (MEAN_PATH and os.path.exists(MEAN_PATH)) else None,\n",
        "    stds_path=STD_PATH if (STD_PATH and os.path.exists(STD_PATH)) else None,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Pipeline cr√©√© avec succ√®s!\")\n",
        "print(f\"   - LR shape: {pipeline.lr_dataset.dims}\")\n",
        "print(f\"   - HR shape: {pipeline.hr_dataset.dims}\")\n",
        "print(f\"   - Residual shape: {pipeline.residual_dataset.dims}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Config test (jeux tenus √† l'√©cart)\n",
        "\n",
        "Configuration des chemins et GCMs pour l'√©valuation sur donn√©es de test (comme downscaling : un jeu par GCM dans `data/raw/test/`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config test : chemins et GCMs (comme downscaling eval_predictor_variables + ground_truth)\n",
        "TEST_ROOT = Path(\"data/raw/test\")\n",
        "\n",
        "# D√©tection des GCMs de test √† partir des fichiers pr√©sents (convention: {GCM}_histupdated_compressed.nc, {GCM}_historical_precip_compressed.nc)\n",
        "def _discover_test_gcms():\n",
        "    if not TEST_ROOT.exists():\n",
        "        return []\n",
        "    lr_suffix = \"_histupdated_compressed.nc\"\n",
        "    hr_suffix = \"_historical_precip_compressed.nc\"\n",
        "    lr_stems = {f.name.replace(lr_suffix, \"\") for f in TEST_ROOT.glob(f\"*{lr_suffix}\")}\n",
        "    hr_stems = {f.name.replace(hr_suffix, \"\") for f in TEST_ROOT.glob(f\"*{hr_suffix}\")}\n",
        "    return sorted(lr_stems & hr_stems)\n",
        "\n",
        "TEST_GCMs = _discover_test_gcms()\n",
        "if not TEST_GCMs:\n",
        "    TEST_GCMs = [\"EC-Earth3\", \"NorESM2-MM\"]  # fallback si dossier vide ou absent\n",
        "\n",
        "def test_lr_path(gcm):\n",
        "    return str(TEST_ROOT / f\"{gcm}_histupdated_compressed.nc\")\n",
        "\n",
        "def test_hr_path(gcm):\n",
        "    return str(TEST_ROOT / f\"{gcm}_historical_precip_compressed.nc\")\n",
        "\n",
        "# V√©rifier existence des fichiers pour chaque GCM\n",
        "print(\"GCMs de test configur√©s:\", TEST_GCMs)\n",
        "for gcm in TEST_GCMs:\n",
        "    lr_ok = Path(test_lr_path(gcm)).exists()\n",
        "    hr_ok = Path(test_hr_path(gcm)).exists()\n",
        "    if lr_ok and hr_ok:\n",
        "        print(f\"  OK {gcm}: LR et HR trouv√©s\")\n",
        "    else:\n",
        "        print(f\"  Manquant {gcm}: LR={lr_ok}, HR={hr_ok}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_test_pipeline(gcm):\n",
        "    \"\"\"\n",
        "    Construit un pipeline de test pour un GCM (m√™me options que le pipeline d'entra√Ænement).\n",
        "    Retourne NetCDFDataPipeline ou None si les fichiers sont manquants.\n",
        "    \"\"\"\n",
        "    lr_path = test_lr_path(gcm)\n",
        "    hr_path = test_hr_path(gcm)\n",
        "    if not Path(lr_path).exists() or not Path(hr_path).exists():\n",
        "        print(f\"  Fichiers test manquants pour {gcm}, skip.\")\n",
        "        return None\n",
        "    try:\n",
        "        return NetCDFDataPipeline(\n",
        "            lr_path=lr_path,\n",
        "            hr_path=hr_path,\n",
        "            static_path=STATIC_PATH,\n",
        "            seq_len=SEQ_LEN,\n",
        "            baseline_strategy=BASELINE_STRATEGY,\n",
        "            baseline_factor=BASELINE_FACTOR,\n",
        "            normalize=NORMALIZE,\n",
        "            nan_fill_strategy=NAN_FILL_STRATEGY,\n",
        "            precipitation_delta=PRECIPITATION_DELTA,\n",
        "            lr_variables=LR_VARIABLES,\n",
        "            hr_variables=HR_VARIABLES,\n",
        "            static_variables=STATIC_VARIABLES,\n",
        "            means_path=MEAN_PATH if (MEAN_PATH and os.path.exists(MEAN_PATH)) else None,\n",
        "            stds_path=STD_PATH if (STD_PATH and os.path.exists(STD_PATH)) else None,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"  Erreur build_test_pipeline({gcm}): {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:29.255704Z",
          "start_time": "2025-11-25T23:19:27.672707Z"
        }
      },
      "outputs": [],
      "source": [
        "# Visualiser baseline vs HR vs residual\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "time_idx = 0\n",
        "hr_var = list(pipeline.hr_dataset.data_vars)[0]\n",
        "\n",
        "# HR truth\n",
        "pipeline.hr_dataset[hr_var].isel(time=time_idx).plot(ax=axes[0], cmap='RdBu_r')\n",
        "axes[0].set_title('HR Ground Truth', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Baseline\n",
        "pipeline.baseline_prepared[hr_var].isel(time=time_idx).plot(ax=axes[1], cmap='RdBu_r')\n",
        "axes[1].set_title('Baseline (Deterministic)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Residual\n",
        "pipeline.residual_dataset[hr_var].isel(time=time_idx).plot(ax=axes[2], cmap='seismic', center=0)\n",
        "axes[2].set_title('Residual (HR - Baseline)', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Cr√©ation du dataset iterable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:30.426965Z",
          "start_time": "2025-11-25T23:19:29.291132Z"
        }
      },
      "outputs": [],
      "source": [
        "# Cr√©er le dataset PyTorch\n",
        "dataset = pipeline.build_sequence_dataset(\n",
        "    seq_len=SEQ_LEN,\n",
        "    stride=1,\n",
        "    as_torch=True,\n",
        ")\n",
        "\n",
        "# Inspecter un batch\n",
        "sample = next(iter(dataset))\n",
        "\n",
        "print(\"üì¶ Structure d'un batch:\")\n",
        "print(\"=\" * 60)\n",
        "for key, value in sample.items():\n",
        "    if torch.is_tensor(value):\n",
        "        print(f\"  {key:15s}: shape={tuple(value.shape)}, dtype={value.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {key:15s}: {type(value).__name__}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset cr√©√© avec succ√®s!\")\n",
        "print(f\"   Longueur de s√©quence: {SEQ_LEN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è 3. Construction du Mod√®le\n",
        "\n",
        "### 3.1 Configuration du mod√®le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:30.471758Z",
          "start_time": "2025-11-25T23:19:30.449078Z"
        }
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dimensions (√† adapter selon vos donn√©es)\n",
        "lr_shape = tuple(sample[\"lr\"].shape[2:4])  # Extraire (lat, lon) de LR\n",
        "hr_shape = tuple(sample[\"residual\"].shape[2:4])  # Extraire (lat, lon) de HR\n",
        "\n",
        "# Architecture (r√©duite pour overfit rapide)\n",
        "HIDDEN_DIM = 32\n",
        "CONDITIONING_DIM = 32\n",
        "RCN_DRIVER_DIM = sample[\"lr\"].shape[1]  # Nombre de channels LR\n",
        "RCN_RECONSTRUCTION_DIM = sample[\"lr\"].shape[1]\n",
        "DIFFUSION_STEPS = 50\n",
        "\n",
        "# Hyperparam√®tres d'entra√Ænement\n",
        "LEARNING_RATE = 5e-5  # R√©duit de 3e-4 √† 5e-5 (6x plus bas) pour √©viter la divergence\n",
        "NUM_EPOCHS = 6\n",
        "GRADIENT_CLIPPING = 0.5  # R√©duit de 1.0 √† 0.5 pour stabiliser l'entra√Ænement\n",
        "\n",
        "# Pond√©ration des pertes (simplifi√©e pour test \"m√©morisation\")\n",
        "LAMBDA_GEN = 1.0   # Perte de diffusion\n",
        "BETA_REC = 0.0     # d√©sactiv√©e pour simplifier\n",
        "GAMMA_DAG = 0.0    # d√©sactiv√©e pour simplifier\n",
        "\n",
        "print(f\"üñ•Ô∏è  Device: {DEVICE}\")\n",
        "print(f\"üìê LR shape: {lr_shape}\")\n",
        "print(f\"üìê HR shape: {hr_shape}\")\n",
        "print(f\"üìê LR channels: {RCN_DRIVER_DIM}\")\n",
        "print(f\"üìê HR channels: {sample['residual'].shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Construction du graph builder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:30.674382Z",
          "start_time": "2025-11-25T23:19:30.515377Z"
        }
      },
      "outputs": [],
      "source": [
        "# Graph builder pour les relations spatiales\n",
        "builder = HeteroGraphBuilder(\n",
        "    lr_shape=lr_shape,\n",
        "    hr_shape=hr_shape,\n",
        "    static_dataset=pipeline.get_static_dataset(),\n",
        "    include_mid_layer=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Graph builder cr√©√©\")\n",
        "print(f\"   Dynamic node types: {builder.dynamic_node_types}\")\n",
        "print(f\"   Static node types: {builder.static_node_types}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Initialisation des modules du mod√®le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:38.298846Z",
          "start_time": "2025-11-25T23:19:30.701567Z"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Intelligible Variable Encoder\n",
        "encoder_configs = [\n",
        "    IntelligibleVariableConfig(\n",
        "        name=\"surface\",\n",
        "        meta_path=(\"GP850\", \"spat_adj\", \"GP850\"),\n",
        "        pool=\"mean\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "if pipeline.get_static_dataset() is not None:\n",
        "    encoder_configs.append(\n",
        "        IntelligibleVariableConfig(\n",
        "            name=\"static\",\n",
        "            meta_path=(\"SP_HR\", \"causes\", \"GP850\"),\n",
        "            pool=\"mean\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "encoder = IntelligibleVariableEncoder(\n",
        "    configs=encoder_configs,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    conditioning_dim=CONDITIONING_DIM,\n",
        ").to(DEVICE)\n",
        "\n",
        "num_vars = len(encoder_configs)\n",
        "print(f\"‚úÖ Encoder cr√©√© avec {num_vars} variables intelligibles\")\n",
        "\n",
        "# 2. Causal RCN\n",
        "rcn_cell = RCNCell(\n",
        "    num_vars=num_vars,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    driver_dim=RCN_DRIVER_DIM,\n",
        "    reconstruction_dim=RCN_RECONSTRUCTION_DIM,\n",
        "    dropout=0.0,\n",
        ").to(DEVICE)\n",
        "\n",
        "rcn_runner = RCNSequenceRunner(rcn_cell, detach_interval=None)\n",
        "print(f\"‚úÖ RCN cr√©√©\")\n",
        "\n",
        "# 3. Diffusion Decoder (UNet r√©duit pour test rapide)\n",
        "hr_channels = sample['residual'].shape[1]\n",
        "\n",
        "# UNet minimal pour accepter des tailles non multiples de 2^n (ex: 23x26)\n",
        "UNET_KWARGS = dict(\n",
        "    layers_per_block=1,\n",
        "    block_out_channels=(32,),\n",
        "    down_block_types=(\"DownBlock2D\",),\n",
        "    up_block_types=(\"UpBlock2D\",),\n",
        "    norm_num_groups=8,\n",
        ")\n",
        "\n",
        "diffusion = CausalDiffusionDecoder(\n",
        "    in_channels=hr_channels,\n",
        "    conditioning_dim=CONDITIONING_DIM,\n",
        "    height=hr_shape[0],\n",
        "    width=hr_shape[1],\n",
        "    num_diffusion_steps=DIFFUSION_STEPS,\n",
        "    unet_kwargs=UNET_KWARGS,\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"‚úÖ Diffusion decoder cr√©√© avec {DIFFUSION_STEPS} steps\")\n",
        "\n",
        "# 4. Optimizer\n",
        "params = list(encoder.parameters()) + list(rcn_cell.parameters()) + list(diffusion.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"‚úÖ Optimizer cr√©√© (lr={LEARNING_RATE})\")\n",
        "\n",
        "# Compter les param√®tres (en g√©rant les param√®tres non initialis√©s/lazy)\n",
        "def count_parameters(model):\n",
        "    \"\"\"Compte les param√®tres d'un mod√®le, en g√©rant les param√®tres non initialis√©s.\"\"\"\n",
        "    total = 0\n",
        "    trainable = 0\n",
        "    for p in model.parameters():\n",
        "        try:\n",
        "            num = p.numel()\n",
        "            total += num\n",
        "            if p.requires_grad:\n",
        "                trainable += num\n",
        "        except (ValueError, RuntimeError):\n",
        "            # Param√®tre non initialis√© (lazy module), on l'ignore pour le moment\n",
        "            pass\n",
        "    return total, trainable\n",
        "\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "for model in [encoder, rcn_cell, diffusion]:\n",
        "    total, trainable = count_parameters(model)\n",
        "    total_params += total\n",
        "    trainable_params += trainable\n",
        "\n",
        "print(f\"\\nüìä Nombre de param√®tres:\")\n",
        "print(f\"   Total: {total_params:,}\")\n",
        "print(f\"   Trainable: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 4. Entra√Ænement du Mod√®le\n",
        "\n",
        "### 4.1 Fonction helper pour la conversion des batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-25T23:19:38.326580Z",
          "start_time": "2025-11-25T23:19:38.308260Z"
        }
      },
      "outputs": [],
      "source": [
        "def convert_sample_to_batch(sample, builder, device):\n",
        "    \"\"\"\n",
        "    Convertit un √©chantillon du DataLoader en format attendu par train_epoch.\n",
        "    \"\"\"\n",
        "    lr_seq = sample[\"lr\"]  # [seq_len, channels, lat, lon]\n",
        "    seq_len = lr_seq.shape[0]\n",
        "    \n",
        "    # Convertir grille LR en nodes\n",
        "    lr_nodes_steps = []\n",
        "    for t in range(seq_len):\n",
        "        lr_nodes_steps.append(builder.lr_grid_to_nodes(lr_seq[t]))\n",
        "    lr_tensor = torch.stack(lr_nodes_steps, dim=0)  # [seq_len, N_lr, channels]\n",
        "    \n",
        "    # Pr√©parer hetero data\n",
        "    dynamic_features = {node_type: lr_nodes_steps[0] for node_type in builder.dynamic_node_types}\n",
        "    hetero = builder.prepare_step_data(dynamic_features).to(device)\n",
        "    \n",
        "    batch = {\n",
        "        \"lr\": lr_tensor,\n",
        "        \"residual\": sample[\"residual\"],\n",
        "        \"baseline\": sample.get(\"baseline\"),\n",
        "        \"hetero\": hetero,\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "def iterate_batches(dataloader, builder, device):\n",
        "    \"\"\"G√©n√©rateur qui convertit chaque sample du dataloader.\"\"\"\n",
        "    for sample in dataloader:\n",
        "        yield convert_sample_to_batch(sample, builder, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Boucle d'entra√Ænement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-11-25T23:19:38.341159Z"
        }
      },
      "outputs": [],
      "source": [
        "# Mode overfit: on fixe UN batch et on le r√©p√®te pour v√©rifier que le mod√®le m√©morise\n",
        "fixed_sample = sample\n",
        "fixed_batch = convert_sample_to_batch(fixed_sample, builder, DEVICE)\n",
        "\n",
        "def repeat_fixed_batch(n_steps: int):\n",
        "    for _ in range(n_steps):\n",
        "        batch = {}\n",
        "        for k, v in fixed_batch.items():\n",
        "            batch[k] = v.to(DEVICE) if torch.is_tensor(v) else v\n",
        "        yield batch\n",
        "\n",
        "history = {k: [] for k in [\"loss\", \"loss_gen\", \"loss_rec\", \"loss_dag\"]}\n",
        "\n",
        "STEPS_PER_EPOCH = 10\n",
        "\n",
        "print(\"üèãÔ∏è D√©but overfit (m√©morisation)...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    metrics = train_epoch(\n",
        "        encoder=encoder,\n",
        "        rcn_runner=rcn_runner,\n",
        "        diffusion_decoder=diffusion,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=repeat_fixed_batch(STEPS_PER_EPOCH),\n",
        "        lambda_gen=LAMBDA_GEN,\n",
        "        beta_rec=BETA_REC,\n",
        "        gamma_dag=GAMMA_DAG,\n",
        "        conditioning_fn=None,\n",
        "        device=DEVICE,\n",
        "        gradient_clipping=GRADIENT_CLIPPING,\n",
        "    )\n",
        "\n",
        "    for key in history.keys():\n",
        "        history[key].append(metrics[key])\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}/{NUM_EPOCHS} | \"\n",
        "        f\"Loss: {metrics['loss']:.4f} | \"\n",
        "        f\"Gen: {metrics['loss_gen']:.4f} | \"\n",
        "        f\"Rec: {metrics['loss_rec']:.4f} | \"\n",
        "        f\"DAG: {metrics['loss_dag']:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Overfit termin√©!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Visualisation de l'entra√Ænement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Courbes de loss\n",
        "if len(history[\"loss\"]) == 0:\n",
        "    raise RuntimeError(\"Aucune m√©trique dans history: entra√Ænement non ex√©cut√©.\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "epochs = range(1, len(history[\"loss\"]) + 1)\n",
        "\n",
        "# Loss totale\n",
        "axes[0, 0].plot(epochs, history['loss'], linewidth=2, color='navy')\n",
        "axes[0, 0].set_title('Total Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss de g√©n√©ration (diffusion)\n",
        "axes[0, 1].plot(epochs, history['loss_gen'], linewidth=2, color='crimson')\n",
        "axes[0, 1].set_title('Generation Loss (Diffusion)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss de reconstruction\n",
        "axes[1, 0].plot(epochs, history['loss_rec'], linewidth=2, color='green')\n",
        "axes[1, 0].set_title('Reconstruction Loss', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Contrainte DAG (NO TEARS)\n",
        "axes[1, 1].plot(epochs, history['loss_dag'], linewidth=2, color='orange')\n",
        "axes[1, 1].set_title('DAG Constraint (NO TEARS)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Target')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Courbes sauvegard√©es dans 'training_curves.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Sauvegarde du mod√®le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarder le checkpoint\n",
        "checkpoint_path = \"st_cdgm_checkpoint.pth\"\n",
        "\n",
        "torch.save({\n",
        "    'epoch': NUM_EPOCHS,\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'rcn_cell_state_dict': rcn_cell.state_dict(),\n",
        "    'diffusion_state_dict': diffusion.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'history': history,\n",
        "    'config': {\n",
        "        'hidden_dim': HIDDEN_DIM,\n",
        "        'conditioning_dim': CONDITIONING_DIM,\n",
        "        'diffusion_steps': DIFFUSION_STEPS,\n",
        "        'lr_shape': lr_shape,\n",
        "        'hr_shape': hr_shape,\n",
        "    }\n",
        "}, checkpoint_path)\n",
        "\n",
        "print(f\"‚úÖ Checkpoint sauvegard√©: {checkpoint_path}\")\n",
        "print(f\"   Taille: {Path(checkpoint_path).stat().st_size / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ 5. √âvaluation et Tests\n",
        "\n",
        "### 5.1 Fonction d'inf√©rence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_prediction(encoder, rcn_runner, diffusion, sample, builder, device):\n",
        "    \"\"\"G√©n√®re une pr√©diction haute r√©solution √† partir d'un √©chantillon.\n",
        "\n",
        "    Note (mode overfit): ici on pr√©dit 1 canal (ex: t_850). La branche\n",
        "    \"contraintes physiques\" du decoder attend 3 canaux (T_min, Œî1, Œî2), donc\n",
        "    on d√©sactive `apply_constraints` pendant le sampling.\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    rcn_runner.cell.eval()\n",
        "    diffusion.eval()\n",
        "\n",
        "    # Convertir le sample\n",
        "    batch = convert_sample_to_batch(sample, builder, device)\n",
        "\n",
        "    lr_data = batch[\"lr\"].to(device)\n",
        "    hetero_data = batch[\"hetero\"]\n",
        "\n",
        "    # Forward pass\n",
        "    H_init = encoder.init_state(hetero_data).to(device)\n",
        "    drivers = [lr_data[t] for t in range(lr_data.shape[0])]\n",
        "    seq_output = rcn_runner.run(H_init, drivers, reconstruction_sources=None)\n",
        "\n",
        "    # Conditioning pour la diffusion\n",
        "    H_condition = seq_output.states[-1]\n",
        "    conditioning = encoder.project_state_tensor(H_condition).to(device)\n",
        "\n",
        "    # G√©n√©ration par diffusion (sampling)\n",
        "    # - apply_constraints=False: compatible sorties 1 canal\n",
        "    generated = diffusion.sample(\n",
        "        conditioning,\n",
        "        num_steps=25,\n",
        "        scheduler_type=\"ddpm\",\n",
        "        apply_constraints=False,\n",
        "    )\n",
        "\n",
        "    return generated.residual, batch[\"residual\"][-1].to(device), batch[\"baseline\"][-1].to(device)\n",
        "\n",
        "print(\"‚úÖ Fonction d'inf√©rence d√©finie\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 G√©n√©ration de pr√©dictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prendre quelques √©chantillons de test\n",
        "test_dataset = pipeline.build_sequence_dataset(seq_len=SEQ_LEN, as_torch=True)\n",
        "test_samples = [next(iter(test_dataset)) for _ in range(3)]  # 3 √©chantillons\n",
        "\n",
        "print(f\"üìä {len(test_samples)} √©chantillons de test pr√©par√©s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# G√©n√©rer des pr√©dictions\n",
        "predictions = []\n",
        "targets = []\n",
        "baselines = []\n",
        "\n",
        "print(\"üîÆ G√©n√©ration des pr√©dictions...\")\n",
        "for i, sample in enumerate(test_samples):\n",
        "    pred, target, baseline = generate_prediction(encoder, rcn_runner, diffusion, sample, builder, DEVICE)\n",
        "    predictions.append(pred.cpu())\n",
        "    targets.append(target.cpu())\n",
        "    baselines.append(baseline.cpu())\n",
        "    print(f\"  Sample {i+1}/{len(test_samples)} trait√©\")\n",
        "\n",
        "print(\"‚úÖ Pr√©dictions g√©n√©r√©es\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic des donn√©es avant calcul des m√©triques\n",
        "print(\"\\nüîç Diagnostic des donn√©es:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, (pred, target, baseline) in enumerate(zip(predictions, targets, baselines)):\n",
        "    pred_squeezed = pred.squeeze(0) if pred.dim() == 4 else pred\n",
        "    full_pred = baselines[i] + pred_squeezed\n",
        "    full_target = baselines[i] + target\n",
        "    \n",
        "    # V√©rifications\n",
        "    pred_nan_count = torch.isnan(full_pred).sum().item()\n",
        "    target_nan_count = torch.isnan(full_target).sum().item()\n",
        "    pred_inf_count = torch.isinf(full_pred).sum().item()\n",
        "    target_inf_count = torch.isinf(full_target).sum().item()\n",
        "    \n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  Pr√©dictions - NaN: {pred_nan_count}, Inf: {pred_inf_count}\")\n",
        "    print(f\"  Cibles - NaN: {target_nan_count}, Inf: {target_inf_count}\")\n",
        "    \n",
        "    if pred_nan_count > 0 or target_nan_count > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  NaN d√©tect√©s - Nettoyage automatique...\")\n",
        "        full_pred = torch.nan_to_num(full_pred, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        full_target = torch.nan_to_num(full_target, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        predictions[i] = full_pred - baselines[i]\n",
        "        targets[i] = full_target - baselines[i]\n",
        "    \n",
        "    # Statistiques sur les valeurs valides\n",
        "    valid_mask = ~torch.isnan(full_target) & ~torch.isnan(full_pred)\n",
        "    if valid_mask.sum() > 0:\n",
        "        pred_valid = full_pred[valid_mask]\n",
        "        target_valid = full_target[valid_mask]\n",
        "        print(f\"  Valeurs valides: {valid_mask.sum().item()}/{valid_mask.numel()}\")\n",
        "        print(f\"  Pr√©diction - Min: {pred_valid.min():.6f}, Max: {pred_valid.max():.6f}, Mean: {pred_valid.mean():.6f}, Std: {pred_valid.std():.6f}\")\n",
        "        print(f\"  Cible - Min: {target_valid.min():.6f}, Max: {target_valid.max():.6f}, Mean: {target_valid.mean():.6f}, Std: {target_valid.std():.6f}\")\n",
        "        \n",
        "        # Avertissement si variance tr√®s faible\n",
        "        if target_valid.std() < 1e-6:\n",
        "            print(f\"  ‚ö†Ô∏è  Variance de la cible tr√®s faible ({target_valid.std():.6e}) - risque de NaN dans corr√©lation\")\n",
        "        if pred_valid.std() < 1e-6:\n",
        "            print(f\"  ‚ö†Ô∏è  Variance de la pr√©diction tr√®s faible ({pred_valid.std():.6e}) - risque de NaN dans corr√©lation\")\n",
        "\n",
        "print(\"\\n‚úÖ Diagnostic termin√©\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Calcul des m√©triques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred, target, mask=None):\n",
        "    \"\"\"\n",
        "    Calcule des m√©triques robustes (MSE, RMSE, MAE, Corr√©lation) en g√©rant les NaNs et la stabilit√©.\n",
        "    \n",
        "    Args:\n",
        "        pred (torch.Tensor): Sorties du mod√®le.\n",
        "        target (torch.Tensor): Donn√©es de v√©rit√© terrain (Ground Truth).\n",
        "        mask (torch.Tensor, optional): Masque bool√©en o√π 1 est valide, 0 est invalide/remplissage.\n",
        "        \n",
        "    Returns:\n",
        "        dict: Dictionnaire contenant les valeurs scalaires des m√©triques.\n",
        "    \"\"\"\n",
        "    # 1. Cr√©er un masque de validit√©\n",
        "    if mask is None:\n",
        "        mask = ~torch.isnan(target) & ~torch.isnan(pred) & ~torch.isinf(target) & ~torch.isinf(pred)\n",
        "    \n",
        "    # 2. Aplatir et Filtrer\n",
        "    pred_flat = pred.flatten()\n",
        "    target_flat = target.flatten()\n",
        "    \n",
        "    if mask.dim() > 0:\n",
        "        mask_flat = mask.flatten()\n",
        "    else:\n",
        "        mask_flat = mask\n",
        "    \n",
        "    pred_valid = pred_flat[mask_flat]\n",
        "    target_valid = target_flat[mask_flat]\n",
        "    \n",
        "    # V√©rifier si nous avons des donn√©es valides\n",
        "    if pred_valid.numel() == 0:\n",
        "        print(\"[WARN] Aucune donn√©e valide pour le calcul des m√©triques\")\n",
        "        return {\n",
        "            \"MSE\": 0.0, \"RMSE\": 0.0, \"MAE\": 0.0, \"Correlation\": 0.0\n",
        "        }\n",
        "\n",
        "    # 3. Calculer les M√©triques de Distance\n",
        "    mse = torch.mean((pred_valid - target_valid) ** 2)\n",
        "    rmse = torch.sqrt(mse)\n",
        "    mae = torch.mean(torch.abs(pred_valid - target_valid))\n",
        "\n",
        "    # 4. Calculer la Corr√©lation de Pearson avec Stabilit√©\n",
        "    vx = pred_valid - torch.mean(pred_valid)\n",
        "    vy = target_valid - torch.mean(target_valid)\n",
        "    \n",
        "    epsilon = 1e-8\n",
        "    numerator = torch.sum(vx * vy)\n",
        "    denominator = torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2))\n",
        "    \n",
        "    if denominator < epsilon:\n",
        "        correlation = torch.tensor(0.0)\n",
        "    else:\n",
        "        correlation = numerator / (denominator + epsilon)\n",
        "\n",
        "    return {\n",
        "        \"MSE\": mse.item(),\n",
        "        \"RMSE\": rmse.item(),\n",
        "        \"MAE\": mae.item(),\n",
        "        \"Correlation\": correlation.item()\n",
        "    }\n",
        "\n",
        "# Calculer les m√©triques pour chaque √©chantillon\n",
        "print(\"\\nüìà M√©triques de performance:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_metrics = []\n",
        "for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
        "    # Reconstruction compl√®te: baseline + residual pr√©dit\n",
        "    # pred peut avoir une dimension batch [1, C, H, W], on la supprime avec squeeze\n",
        "    pred_squeezed = pred.squeeze(0) if pred.dim() == 4 else pred\n",
        "    full_pred = baselines[i] + pred_squeezed\n",
        "    full_target = baselines[i] + target\n",
        "    \n",
        "    metrics = compute_metrics(full_pred, full_target)\n",
        "    all_metrics.append(metrics)\n",
        "    \n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name:15s}: {value:.6f}\")\n",
        "\n",
        "# Moyennes\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä Moyennes sur tous les √©chantillons:\")\n",
        "avg_metrics = {key: np.mean([m[key] for m in all_metrics]) for key in all_metrics[0].keys()}\n",
        "for metric_name, value in avg_metrics.items():\n",
        "    print(f\"  {metric_name:15s}: {value:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 √âvaluation sur jeux de test par GCM\n",
        "\n",
        "√âvaluation sur donn√©es tenues √† l'√©cart (data/raw/test/), un pipeline par GCM, m√™me normalisation que l'entra√Ænement. Inf√©rence puis m√©triques par GCM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mode √©valuation et nombre d'√©chantillons de test par GCM\n",
        "encoder.eval()\n",
        "rcn_runner.cell.eval()\n",
        "diffusion.eval()\n",
        "N_TEST_SAMPLES = 10  # nombre d'√©chantillons √† √©valuer par GCM (ajuster si besoin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boucle de test par GCM (donn√©es tenues √† l'√©cart)\n",
        "test_results = {}\n",
        "for gcm in TEST_GCMs:\n",
        "    pipe_test = build_test_pipeline(gcm)\n",
        "    if pipe_test is None:\n",
        "        continue\n",
        "    test_dataset = pipe_test.build_sequence_dataset(seq_len=SEQ_LEN, as_torch=True)\n",
        "    # Prendre au plus N_TEST_SAMPLES √©chantillons\n",
        "    test_samples_gcm = []\n",
        "    it = iter(test_dataset)\n",
        "    for _ in range(N_TEST_SAMPLES):\n",
        "        try:\n",
        "            test_samples_gcm.append(next(it))\n",
        "        except StopIteration:\n",
        "            break\n",
        "    if not test_samples_gcm:\n",
        "        print(f\"  Aucun √©chantillon pour {gcm}, skip.\")\n",
        "        continue\n",
        "    predictions_gcm, targets_gcm, baselines_gcm = [], [], []\n",
        "    for sample in test_samples_gcm:\n",
        "        pred, target, baseline = generate_prediction(encoder, rcn_runner, diffusion, sample, builder, DEVICE)\n",
        "        predictions_gcm.append(pred.cpu())\n",
        "        targets_gcm.append(target.cpu())\n",
        "        baselines_gcm.append(baseline.cpu())\n",
        "    # M√©triques par √©chantillon puis moyenne pour ce GCM\n",
        "    all_metrics_gcm = []\n",
        "    for i, (pred, target) in enumerate(zip(predictions_gcm, targets_gcm)):\n",
        "        pred_sq = pred.squeeze(0) if pred.dim() == 4 else pred\n",
        "        full_pred = baselines_gcm[i] + pred_sq\n",
        "        full_target = baselines_gcm[i] + target\n",
        "        full_pred = torch.nan_to_num(full_pred, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        full_target = torch.nan_to_num(full_target, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        m = compute_metrics(full_pred, full_target)\n",
        "        all_metrics_gcm.append(m)\n",
        "    avg_gcm = {k: np.mean([m[k] for m in all_metrics_gcm]) for k in all_metrics_gcm[0].keys()}\n",
        "    test_results[gcm] = avg_gcm\n",
        "    print(f\"  {gcm}: MSE={avg_gcm['MSE']:.6f}, MAE={avg_gcm['MAE']:.6f}, Corr={avg_gcm['Correlation']:.6f}\")\n",
        "print(\"√âvaluation par GCM termin√©e.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affichage des m√©triques par GCM et export CSV\n",
        "if test_results:\n",
        "    metrics_df = pd.DataFrame(test_results).T\n",
        "    metrics_df.index.name = \"GCM\"\n",
        "    print(\"M√©triques par GCM (jeux de test):\")\n",
        "    print(metrics_df.to_string())\n",
        "    out_csv = \"test_metrics_by_gcm.csv\"\n",
        "    metrics_df.to_csv(out_csv)\n",
        "    print(f\"\\nM√©triques export√©es dans '{out_csv}'\")\n",
        "else:\n",
        "    print(\"Aucun r√©sultat de test (test_results vide). V√©rifier TEST_GCMs et les fichiers dans data/raw/test/.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 6. Visualisation des R√©sultats\n",
        "\n",
        "### 6.1 Comparaison visuelle: Baseline vs Pr√©diction vs Ground Truth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiser les r√©sultats pour le premier √©chantillon\n",
        "# Import n√©cessaire pour CenteredNorm (centrage de colormap)\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "sample_idx = 0\n",
        "\n",
        "# predictions[sample_idx] est maintenant directement un tenseur [batch, C, H, W] ou [C, H, W]\n",
        "# On squeeze pour enlever la dimension batch si elle existe\n",
        "pred_residual = predictions[sample_idx].squeeze().numpy()  # [C, H, W]\n",
        "target_residual = targets[sample_idx].squeeze().numpy()\n",
        "baseline = baselines[sample_idx].squeeze().numpy()\n",
        "\n",
        "# Reconstruction compl√®te\n",
        "full_pred = baseline + pred_residual\n",
        "full_target = baseline + target_residual\n",
        "\n",
        "# S√©lectionner le premier canal si multi-canal\n",
        "if len(full_pred.shape) > 2:\n",
        "    channel_idx = 0\n",
        "    pred_plot = full_pred[channel_idx]\n",
        "    target_plot = full_target[channel_idx]\n",
        "    baseline_plot = baseline[channel_idx]\n",
        "else:\n",
        "    pred_plot = full_pred\n",
        "    target_plot = full_target\n",
        "    baseline_plot = baseline\n",
        "\n",
        "# Cr√©er la visualisation\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Trouver les valeurs min/max pour une √©chelle commune\n",
        "vmin = min(baseline_plot.min(), pred_plot.min(), target_plot.min())\n",
        "vmax = max(baseline_plot.max(), pred_plot.max(), target_plot.max())\n",
        "\n",
        "# Premi√®re ligne: Baseline, Pr√©diction, Ground Truth\n",
        "im1 = axes[0, 0].imshow(baseline_plot, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
        "axes[0, 0].set_title('Baseline (Deterministic)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].axis('off')\n",
        "plt.colorbar(im1, ax=axes[0, 0], fraction=0.046)\n",
        "\n",
        "im2 = axes[0, 1].imshow(pred_plot, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
        "axes[0, 1].set_title('Prediction (Baseline + Diffusion)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].axis('off')\n",
        "plt.colorbar(im2, ax=axes[0, 1], fraction=0.046)\n",
        "\n",
        "im3 = axes[0, 2].imshow(target_plot, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
        "axes[0, 2].set_title('Ground Truth (HR)', fontsize=14, fontweight='bold')\n",
        "axes[0, 2].axis('off')\n",
        "plt.colorbar(im3, ax=axes[0, 2], fraction=0.046)\n",
        "\n",
        "# Deuxi√®me ligne: Erreurs\n",
        "baseline_error = np.abs(baseline_plot - target_plot)\n",
        "pred_error = np.abs(pred_plot - target_plot)\n",
        "error_diff = baseline_error - pred_error  # Positive = am√©lioration\n",
        "\n",
        "im4 = axes[1, 0].imshow(baseline_error, cmap='Reds')\n",
        "axes[1, 0].set_title(f'Baseline Error (MAE={baseline_error.mean():.4f})', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].axis('off')\n",
        "plt.colorbar(im4, ax=axes[1, 0], fraction=0.046)\n",
        "\n",
        "im5 = axes[1, 1].imshow(pred_error, cmap='Reds')\n",
        "axes[1, 1].set_title(f'Prediction Error (MAE={pred_error.mean():.4f})', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].axis('off')\n",
        "plt.colorbar(im5, ax=axes[1, 1], fraction=0.046)\n",
        "\n",
        "# CORRECTION: Utiliser CenteredNorm pour centrer la colormap 'seismic' sur 0\n",
        "# matplotlib.imshow() n'accepte pas l'argument 'center' (c'est une fonctionnalit√© de seaborn.heatmap)\n",
        "im6 = axes[1, 2].imshow(error_diff, cmap='seismic', norm=mcolors.CenteredNorm(vcenter=0))\n",
        "axes[1, 2].set_title('Improvement (Baseline - Prediction)', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].axis('off')\n",
        "plt.colorbar(im6, ax=axes[1, 2], fraction=0.046)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('prediction_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualisation sauvegard√©e dans 'prediction_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarder les m√©triques dans un fichier CSV\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "metrics_df['sample'] = range(1, len(all_metrics) + 1)\n",
        "metrics_df = metrics_df[['sample'] + [col for col in metrics_df.columns if col != 'sample']]\n",
        "\n",
        "# Ajouter la ligne des moyennes\n",
        "avg_row = pd.DataFrame([avg_metrics])\n",
        "avg_row.insert(0, 'sample', 'Average')\n",
        "metrics_df = pd.concat([metrics_df, avg_row], ignore_index=True)\n",
        "\n",
        "metrics_df.to_csv('evaluation_metrics.csv', index=False)\n",
        "print(\"‚úÖ M√©triques sauvegard√©es dans 'evaluation_metrics.csv'\")\n",
        "print(\"\\n\", metrics_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 7. R√©sum√© et Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ R√âSUM√â DE L'EXP√âRIENCE ST-CDGM\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nüìÅ Configuration:\")\n",
        "print(f\"   - LR dataset: {LR_PATH}\")\n",
        "print(f\"   - HR dataset: {HR_PATH}\")\n",
        "print(f\"   - Sequence length: {SEQ_LEN}\")\n",
        "print(f\"   - LR shape: {lr_shape}\")\n",
        "print(f\"   - HR shape: {hr_shape}\")\n",
        "\n",
        "print(f\"\\nüèóÔ∏è  Architecture:\")\n",
        "print(f\"   - Hidden dimension: {HIDDEN_DIM}\")\n",
        "print(f\"   - Conditioning dimension: {CONDITIONING_DIM}\")\n",
        "print(f\"   - Diffusion steps: {DIFFUSION_STEPS}\")\n",
        "print(f\"   - Total parameters: {total_params:,}\")\n",
        "\n",
        "print(f\"\\nüöÄ Entra√Ænement:\")\n",
        "print(f\"   - Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Final total loss: {history['loss'][-1]:.6f}\")\n",
        "print(f\"   - Final gen loss: {history['loss_gen'][-1]:.6f}\")\n",
        "print(f\"   - Final rec loss: {history['loss_rec'][-1]:.6f}\")\n",
        "print(f\"   - Final DAG loss: {history['loss_dag'][-1]:.6f}\")\n",
        "\n",
        "print(f\"\\nüìä Performance (moyennes):\")\n",
        "for metric_name, value in avg_metrics.items():\n",
        "    print(f\"   - {metric_name}: {value:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Notebook ex√©cut√© avec succ√®s!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Notes et Prochaines √âtapes\n",
        "\n",
        "### Interpr√©tation des r√©sultats :\n",
        "\n",
        "1. **Loss totale** : Devrait diminuer au fil des epochs\n",
        "2. **Loss de g√©n√©ration (diffusion)** : Mesure la qualit√© du mod√®le de diffusion\n",
        "3. **Loss de reconstruction** : Mesure la capacit√© √† reconstruire les inputs LR\n",
        "4. **DAG constraint (NO TEARS)** : Devrait tendre vers 0 pour assurer l'acyclicit√© du graphe causal\n",
        "\n",
        "### Am√©liorations possibles :\n",
        "\n",
        "- Augmenter le nombre d'epochs (actuellement 20)\n",
        "- Ajuster les hyperparam√®tres (learning rate, hidden dim, etc.)\n",
        "- Essayer diff√©rentes strat√©gies de baseline\n",
        "- Ajouter des transformations de donn√©es (log, normalisation)\n",
        "- Impl√©menter un scheduler de learning rate\n",
        "- Ajouter de la validation pendant l'entra√Ænement\n",
        "\n",
        "### Pour aller plus loin :\n",
        "\n",
        "- Analyser les graphes causaux appris (matrices A)\n",
        "- Comparer avec d'autres m√©thodes de downscaling\n",
        "- √âvaluer sur diff√©rentes r√©gions g√©ographiques\n",
        "- Tester la robustesse aux conditions extr√™mes\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
