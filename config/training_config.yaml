# Hydra Training Configuration for ST-CDGM
# This configuration includes all new optimizations (Phase C, D, E)
#
# FOR CYVERSE VICE USERS:
# - See config/training_config_vice.yaml for VICE-optimized configuration
# - See CYVERSE_VICE_SETUP.md for installation and usage instructions
# - Data Store paths (~/data-store/home/<username>/) are slower for large files
# - Recommended: Copy data to local disk first using scripts/sync_datastore.py

defaults:
  - _self_

# Data Configuration
# 
# For CyVerse VICE users:
# - Option 1 (RECOMMENDED): Copy data to ~/climate_data/data/raw/ first
#   Use relative paths: "data/raw/your_file.nc"
# - Option 2: Use Data Store paths directly (slower):
#   "~/data-store/home/<username>/data/raw/your_file.nc"
data:
  dataset_format: "zarr"  # Format de données: "netcdf", "zarr", ou "shard"
  lr_path: "data/raw/train/predictor_ACCESS-CM2_hist.nc"
  hr_path: "data/raw/train/pr_ACCESS-CM2_hist.nc"
  static_path: "data/raw/static_predictors/ERA5_eval_ccam_12km.198110_NZ_Invariant.nc"
  zarr_dir: "data/raw/train/zarr"  # Répertoire pour données Zarr préprocessées
  shard_dir: "data/raw/train/shards"  # Répertoire pour shards WebDataset
  seq_len: 8  # Increased for 256 Go RAM
  stride: 2  # Faster dataset iteration
  baseline_strategy: "hr_smoothing"  # or "lr_interp"
  baseline_factor: 4
  normalize: true
  target_transform: "log1p"  # ["log1p", "none"]
  nan_fill_strategy: "mean"  # ["zero", "mean", "interpolate"]
  precipitation_delta: 0.01  
  lr_variables: ['u_850', 'u_500', 'u_250', 'v_850', 'v_500', 'v_250', 'w_850', 'w_500', 'w_250', 'q_850', 'q_500', 'q_250', 't_850', 't_500', 't_250']
  hr_variables: ["pr"]
  static_variables: ["orog", "he", "vegt"]

# Graph Configuration
graph:
  lr_shape: [23, 26]
  hr_shape: [172, 179]
  static_variables: []
  include_mid_layer: false

# Encoder Configuration
encoder:
  hidden_dim: 128
  conditioning_dim: 128
  metapaths:
    - name: "GP850_spat_adj"
      src: "GP850"
      relation: "spat_adj"
      target: "GP850"
      pool: "mean"
    - name: "GP850_to_GP500"
      src: "GP850"
      relation: "vert_adj"
      target: "GP500"
      pool: "mean"
    - name: "GP500_spat_adj"
      src: "GP500"
      relation: "spat_adj"
      target: "GP500"
      pool: "mean"
    - name: "GP500_to_GP250"
      src: "GP500"
      relation: "vert_adj"
      target: "GP250"
      pool: "mean"
    - name: "GP250_spat_adj"
      src: "GP250"
      relation: "spat_adj"
      target: "GP250"
      pool: "mean"

# RCN Configuration
rcn:
  hidden_dim: 128
  driver_dim: 8
  reconstruction_dim: 8
  dropout: 0.0
  detach_interval: null

# Diffusion Decoder Configuration
diffusion:
  in_channels: 3
  conditioning_dim: 128  # must match encoder.conditioning_dim
  height: 172
  width: 179
  steps: 100  # Increased for better quality (GPUs handle this well)
  scheduler_type: "ddpm"  # "ddpm", "edm", or "dpm_solver++"
  use_gradient_checkpointing: false  # Phase C3: Gradient checkpointing

# Loss Configuration
loss:
  lambda_gen: 1.0
  beta_rec: 0.1
  gamma_dag: 0.1
  lambda_phy: 0.0  # Phase B2: Physical loss weight
  dag_method: "dagma"  # "dagma" or "no_tears"
  dagma_s: 1.0
  # Phase D1: Focal Loss
  use_focal_loss: false
  focal_alpha: 1.0
  focal_gamma: 2.0
  # Phase D2: Extreme Loss Weighting
  extreme_weight_factor: 0.0
  extreme_percentiles: [95.0, 99.0]
  # Phase D3: DAG Stabilization
  dag_l1_regularization: false
  dag_l1_weight: 0.01
  # Phase D4: Reconstruction Loss Type
  reconstruction_loss_type: "mse"  # "mse", "cosine", or "mse+cosine"

# Training Configuration
training:
  device: "cuda"  # "cuda" or "cpu" - using GPU for multi-GPU training
  epochs: 100  
  batch_size: 48  # Optimized for 64 CPU / 4 GPU (moderate increase for 128GB RAM)
  lr: 0.0002  # Increased for larger batch size
  gradient_clipping: 1.0
  log_every: 10
  use_amp: true  # Mixed Precision for GPU performance
  
  # Multi-GPU configuration (4 GPUs available)
  multi_gpu:
    enabled: true
    strategy: "ddp"  # DistributedDataParallel (better than DataParallel)
    gpus: [0, 1, 2, 3]  # Use all 4 GPUs
    find_unused_parameters: true  # Required for PyG HeteroData graphs
  early_stopping:
    enabled: false
    patience: 7
    min_delta: 0.0
    restore_best: true
  lr_scheduler:
    enabled: false
    mode: "min"  # "min" or "max"
    factor: 0.5 
    patience: 3  
    min_lr: 1e-7
  physical_loss:
    use_predicted_output: false
    physical_sample_interval: 10
    physical_num_steps: 15
  compile:
    enabled: true  # torch.compile for performance (PyTorch 2.0+)
    rcn_mode: "reduce-overhead"
    diffusion_mode: "max-autotune"
    encoder_mode: "reduce-overhead"

# Model Checkpointing
checkpoint:
  enabled: true
  save_dir: "models"
  save_every: 5  
  save_best: true  
  max_checkpoints: 5  

# Evaluation Configuration
evaluation:
  enabled: true
  eval_every: 5  
  num_samples: 10  
  compute_f1_extremes: true
  f1_percentiles: [95.0, 99.0]
  save_visualizations: true
  output_dir: "results"

