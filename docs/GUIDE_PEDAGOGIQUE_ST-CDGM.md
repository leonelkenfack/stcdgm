# Guide PÃ©dagogique : Comprendre le ModÃ¨le ST-CDGM

**Pour les non-initiÃ©s**

---

## ğŸ¯ Introduction : Qu'est-ce que nous essayons de faire ?

Imaginez que vous avez une photo floue d'un paysage. Vous pouvez deviner les grandes lignes (montagnes, ciel, forÃªt), mais les dÃ©tails sont manquants. Notre modÃ¨le ST-CDGM fait exactement cela, mais avec des donnÃ©es mÃ©tÃ©orologiques !

**Le problÃ¨me** : Les modÃ¨les climatiques nous donnent des cartes mÃ©tÃ©o "grossiÃ¨res" (basse rÃ©solution), comme une photo pixelisÃ©e. Par exemple, une grille de 23Ã—26 points pour toute une rÃ©gion.

**Notre solution** : Transformer ces cartes grossiÃ¨res en cartes dÃ©taillÃ©es (haute rÃ©solution) de 172Ã—179 points, avec tous les dÃ©tails fins comme les nuages, les fronts mÃ©tÃ©o, etc.

**Pourquoi c'est important ?** Les prÃ©visions locales prÃ©cises sont essentielles pour l'agriculture, la gestion des catastrophes naturelles, et la planification urbaine.

---

## ğŸ“¦ Ã‰tape 1 : DATA (PrÃ©paration des DonnÃ©es)

### ğŸ¯ But
PrÃ©parer les donnÃ©es mÃ©tÃ©o brutes pour qu'elles soient utilisables par l'intelligence artificielle.

### ğŸ” Analogie
C'est comme prÃ©parer des ingrÃ©dients avant de cuisiner : laver, couper, mesurer. Vous ne pouvez pas cuisiner avec des lÃ©gumes non lavÃ©s et des quantitÃ©s approximatives !

### ğŸ“ Exemple Concret

**Avant (donnÃ©es brutes)** :
```
Fichier NetCDF : NorESM2-MM_histupdated.nc
â”œâ”€ TempÃ©rature Ã  850 hPa : entre 251K et 299K
â”œâ”€ Vent U Ã  850 hPa : entre -37 m/s et +39 m/s
â”œâ”€ 7300 jours de donnÃ©es (1986-2005)
â””â”€ Grille 23Ã—26 points
```

**AprÃ¨s normalisation** :
```
TempÃ©ratures normalisÃ©es : entre -2.5 et +2.5 (Ã©carts-types)
Vents normalisÃ©s : entre -2.5 et +2.5
```

### ğŸ”§ Ce qui se passe Ã©tape par Ã©tape

1. **Normalisation** : 
   - On calcule la moyenne et l'Ã©cart-type de chaque variable.
   - On transforme les valeurs pour qu'elles soient centrÃ©es autour de 0.
   - **Pourquoi ?** L'IA apprend mieux avec des nombres de taille similaire.

2. **SÃ©quenÃ§age** :
   - On dÃ©coupe les donnÃ©es en "fenÃªtres" de 6 jours consÃ©cutifs.
   - **Analogie** : Comme regarder un film par sÃ©quences de 6 images pour comprendre le mouvement.

3. **CrÃ©ation de la Baseline** :
   - On crÃ©e une version "floue" de la haute rÃ©solution par interpolation.
   - **Exemple** : Si on a 23Â°C au point A et 25Â°C au point B, on estime 24Â°C au milieu.
   - **Important** : L'IA apprendra uniquement Ã  ajouter les dÃ©tails manquants (le "rÃ©sidu"), pas Ã  tout refaire !

### âš™ï¸ HyperparamÃ¨tres (RÃ©glages)

| ParamÃ¨tre | Valeur | Ã€ quoi Ã§a sert ? |
|:----------|:-------|:-----------------|
| `seq_len` | 6 jours | Combien de jours consÃ©cutifs on regarde Ã  la fois |
| `normalize` | Oui | Mettre les donnÃ©es Ã  la mÃªme Ã©chelle |
| `baseline_strategy` | "hr_smoothing" | Comment crÃ©er la version floue de rÃ©fÃ©rence |

---

## ğŸ•¸ï¸ Ã‰tape 2 : GRAPH BUILDER (Construction du RÃ©seau)

### ğŸ¯ But
Transformer une simple grille de points en un rÃ©seau intelligent oÃ¹ chaque point "connaÃ®t" ses voisins.

### ğŸ” Analogie
Imaginez une carte de France avec toutes les villes. Au lieu de voir juste des points isolÃ©s, on crÃ©e un rÃ©seau oÃ¹ Paris est connectÃ© Ã  ses villes voisines (Versailles, OrlÃ©ans, etc.). Cela permet de comprendre que ce qui se passe Ã  Paris peut influencer Versailles !

### ğŸ“ Exemple Pas-Ã -Pas

**EntrÃ©e** : Une grille mÃ©tÃ©o de 23 lignes Ã— 26 colonnes = 598 points

**Ã‰tape 1 - CrÃ©er les nÅ“uds** :
```
Point (0,0) â†’ NÅ“ud #0
Point (0,1) â†’ NÅ“ud #1
Point (0,2) â†’ NÅ“ud #2
...
Point (22,25) â†’ NÅ“ud #597
```

**Ã‰tape 2 - CrÃ©er les connexions** :

Pour chaque point, on le connecte Ã  ses 8 voisins (comme les cases autour d'une case d'Ã©checs) :

```
Exemple pour le Point (5, 10) :

        [4,9]  [4,10]  [4,11]
          â†–      â†‘      â†—
        [5,9] â† [5,10] â†’ [5,11]
          â†™      â†“      â†˜
        [6,9]  [6,10]  [6,11]

Ce point a donc 8 connexions
```

**Ã‰tape 3 - Ajouter les relations causales** :
- On connecte aussi les variables statiques (topographie) aux variables dynamiques (mÃ©tÃ©o).
- **Exemple** : Une montagne influence le vent et les prÃ©cipitations autour d'elle.

**Sortie** : 
- 598 nÅ“uds (points mÃ©tÃ©o)
- ~4,700 connexions spatiales (voisins)
- 30,788 nÅ“uds statiques pour la haute rÃ©solution

### âš™ï¸ HyperparamÃ¨tres

| ParamÃ¨tre | Valeur | Ã€ quoi Ã§a sert ? |
|:----------|:-------|:-----------------|
| `lr_shape` | (23, 26) | Taille de la grille basse rÃ©solution |
| `hr_shape` | (172, 179) | Taille de la grille haute rÃ©solution cible |
| `include_mid_layer` | Non | Inclure ou non les niveaux atmosphÃ©riques intermÃ©diaires |

---

## ğŸ§  Ã‰tape 3 : ENCODEUR (Extraction de Patterns MÃ©tÃ©o)

### ğŸ¯ But
Identifier les structures mÃ©tÃ©orologiques importantes dans les donnÃ©es brutes, comme un mÃ©tÃ©orologue expÃ©rimentÃ© qui regarde une carte.

### ğŸ” Analogie
Un mÃ©decin expÃ©rimentÃ© peut regarder une radio et immÃ©diatement identifier "fracture", "inflammation", etc. L'encodeur fait pareil avec les cartes mÃ©tÃ©o : il dÃ©tecte "anticyclone", "front froid", "zone de basse pression", etc.

### ğŸ“ Exemple Concret

**EntrÃ©e** : 598 points avec 15 variables chacun (tempÃ©rature, vent, humiditÃ©...)
```
Point #0 : T=275K, U=5m/s, V=-2m/s, Q=0.004...
Point #1 : T=274K, U=6m/s, V=-1m/s, Q=0.005...
...
```

**Processus - Le GNN (Graph Neural Network) analyse** :
1. Il regarde chaque point et ses voisins.
2. Il dÃ©tecte des patterns :
   - "Zone de haute pression au centre" (tempÃ©ratures Ã©levÃ©es, vents divergents)
   - "Front froid qui descend du nord" (gradient de tempÃ©rature fort)
   - "HumiditÃ© Ã©levÃ©e prÃ¨s de l'ocÃ©an" (gradient d'humiditÃ©)

**Sortie - H(0) : Ã‰tat Initial** :
```
3 variables intelligibles (rÃ©sumÃ©s) :
â”œâ”€ Variable 1 "Advection" : [598 valeurs de 128 dimensions]
â”œâ”€ Variable 2 "Convection" : [598 valeurs de 128 dimensions]
â””â”€ Variable 3 "Influence statique" : [598 valeurs de 128 dimensions]

Forme finale : [3, 598, 128]
```

### ğŸ’¡ Ce que fait vraiment l'encodeur

Au lieu de garder 15 variables brutes dÃ©sorganisÃ©es, il crÃ©e 3 "rÃ©sumÃ©s intelligents" qui capturent :
- **Advection** : Le transport horizontal (vent qui dÃ©place l'air chaud/froid)
- **Convection** : Les mouvements verticaux (air qui monte/descend)
- **Influence statique** : L'effet de la topographie (montagnes, ocÃ©ans)

### âš™ï¸ HyperparamÃ¨tres

| ParamÃ¨tre | Valeur | Ã€ quoi Ã§a sert ? |
|:----------|:-------|:-----------------|
| `hidden_dim` | 128 | Taille des "rÃ©sumÃ©s" pour chaque point |
| `metapaths` | 3 chemins | Combien de types de relations mÃ©tÃ©o on veut capturer |

---

## ğŸ”„ Ã‰tape 4 : RCN (PrÃ©diction de l'Ã‰volution Temporelle)

### ğŸ¯ But
Comprendre comment la mÃ©tÃ©o Ã©volue dans le temps, en respectant les lois de cause Ã  effet.

### ğŸ” Analogie
Imaginez un jeu d'Ã©checs : vous devez prÃ©dire le coup suivant en comprenant comment les piÃ¨ces s'influencent mutuellement (le fou menace la tour, le cavalier protÃ¨ge le roi, etc.) ET en vous souvenant de ce qui s'est passÃ© avant. Le RCN fait exactement Ã§a avec la mÃ©tÃ©o !

### ğŸ“ Les Concepts ClÃ©s ExpliquÃ©s

#### 1. **H(0) - L'Ã‰tat Initial**
**C'est quoi ?** La "photo" de la situation mÃ©tÃ©o au dÃ©part (temps 0).
**Exemple** : 
```
H(0) = [
  Advection : "Vent d'ouest dominant",
  Convection : "Air stable, peu de mouvements verticaux",
  Statique : "Influence des Alpes au sud"
]
```

#### 2. **Driver - Les Nouvelles Observations**
**C'est quoi ?** Les nouvelles donnÃ©es qui arrivent Ã  chaque instant.
**Exemple** :
```
Driver au jour 1 : "La tempÃ©rature a augmentÃ© de 2Â°C"
Driver au jour 2 : "Le vent s'est renforcÃ© Ã  15 m/s"
```
**Analogie** : Comme les nouvelles informations qu'un mÃ©decin reÃ§oit pour ajuster son diagnostic.

#### 3. **H(t) â†’ H(t+1) - L'Ã‰volution**
**C'est quoi ?** Comment on passe de l'Ã©tat actuel Ã  l'Ã©tat suivant.
**Exemple** :
```
Hier (t=0) : "Anticyclone stable"
      â†“
Aujourd'hui (t=1) : "Anticyclone qui se dÃ©place vers l'est"
      â†“
Demain (t=2) : "DÃ©but de dÃ©pression atlantique"
```

#### 4. **SCM - Les RÃ¨gles de Cause Ã  Effet**
**C'est quoi ?** SCM = Structural Causal Model (ModÃ¨le Causal Structurel)
**Son rÃ´le** : Apprendre les relations de cause Ã  effet entre phÃ©nomÃ¨nes mÃ©tÃ©o.

**Exemple concret** :
```
Cause â†’ Effet :
"Haute pression" â†’ "Temps sec"
"Basse pression" â†’ "Risque de pluie"
"DiffÃ©rence de pression" â†’ "Vent fort"
```

**Comment Ã§a marche ?** Le SCM utilise une matrice A_dag (comme un tableau de relations) :
```
            Pression   Vent   TempÃ©rature
Pression    [   0      0.8      0.3     ]
Vent        [   0       0       0.2     ]
TempÃ©rature [  0.1      0        0      ]

Lecture : "La pression influence le vent (0.8) et la tempÃ©rature (0.3)"
```

#### 5. **GRU - La MÃ©moire du SystÃ¨me**
**C'est quoi ?** GRU = Gated Recurrent Unit (UnitÃ© RÃ©currente Ã  Portes)
**Son rÃ´le** : Se souvenir de ce qui s'est passÃ© avant.

**Analogie** : Imaginez que vous prÃ©disez la mÃ©tÃ©o de demain :
- Sans mÃ©moire : Vous regardez juste aujourd'hui.
- Avec GRU : Vous vous souvenez que depuis 3 jours il fait de plus en plus chaud â†’ tendance Ã  la hausse !

**Exemple** :
```
Jour 1 : T = 15Â°C â†’ GRU retient "tendance stable"
Jour 2 : T = 17Â°C â†’ GRU retient "tendance Ã  la hausse"
Jour 3 : T = 19Â°C â†’ GRU prÃ©dit "probablement 21Â°C demain"
```

#### 6. **Pooling - RÃ©sumer l'Information Spatiale**
**C'est quoi ?** RÃ©duire 598 points en un rÃ©sumÃ© unique.
**Exemple** :
```
Avant pooling (598 points) :
[12.5, 12.3, 12.7, 13.1, 12.9, ... 597 autres valeurs]

AprÃ¨s pooling (1 valeur) :
Moyenne = 12.8Â°C â†’ "TempÃ©rature moyenne de la rÃ©gion"
```

**Pourquoi ?** La diffusion (Ã©tape suivante) a besoin d'un rÃ©sumÃ© compact, pas de 598 valeurs sÃ©parÃ©es.

#### 7. **Projection - Adapter le Format**
**C'est quoi ?** Transformer le rÃ©sumÃ© pour qu'il soit compatible avec le module suivant.
**Analogie** : Comme convertir un fichier Word en PDF pour l'envoyer par email.
**Technique** : Passage de [3, 598, 128] â†’ [1, 3, 128] (via pooling + projection linÃ©aire)

### ğŸ”„ Le Cycle Complet du RCN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jour 0 : H(0) "Anticyclone stable"             â”‚
â”‚     â†“                                             â”‚
â”‚  Driver Jour 1 : "Nouvelle observation"          â”‚
â”‚     â†“                                             â”‚
â”‚  SCM : "La pression influence le vent"           â”‚
â”‚     â†“                                             â”‚
â”‚  GRU : "Je me souviens que hier il faisait beau" â”‚
â”‚     â†“                                             â”‚
â”‚  Jour 1 : H(1) "Anticyclone qui se dÃ©place"     â”‚
â”‚     â†“                                             â”‚
â”‚  [Boucle continue pour jours 2, 3, 4, 5...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‰ Fonctions de Perte (Comment le RCN Apprend)

#### L_rec (Reconstruction Loss)
**But** : S'assurer que le RCN garde l'information importante des donnÃ©es d'entrÃ©e.
**Comment ?** On demande au RCN de reconstruire les observations originales Ã  partir de son Ã©tat interne.
**Analogie** : Comme un test oÃ¹ un Ã©tudiant doit restituer ce qu'il a appris pour prouver qu'il a compris.

```
Observation rÃ©elle : TempÃ©rature = 15Â°C
Reconstruction RCN : TempÃ©rature = 14.8Â°C
Erreur = |15 - 14.8| = 0.2Â°C
â†’ Plus l'erreur est petite, mieux c'est !
```

#### L_dag (Contrainte Causale)
**But** : Forcer le modÃ¨le Ã  apprendre des relations causales cohÃ©rentes (pas de cycles).
**Exemple de problÃ¨me** :
```
âŒ MAUVAIS (cycle) :
"A cause B" â†’ "B cause C" â†’ "C cause A" 
(impossible en physique !)

âœ… BON (pas de cycle) :
"Pression cause Vent" â†’ "Vent cause Vagues"
```

### âš™ï¸ HyperparamÃ¨tres

| ParamÃ¨tre | Valeur | Ã€ quoi Ã§a sert ? |
|:----------|:-------|:-----------------|
| `hidden_dim` | 128 | Taille de la mÃ©moire par variable |
| `driver_dim` | 15 | Nombre de variables d'entrÃ©e |
| `num_vars` | 3 | Nombre de phÃ©nomÃ¨nes mÃ©tÃ©o suivis (advection, convection, statique) |
| `dropout` | 0.0 | RÃ©gularisation (0 = pas d'oubli volontaire) |

---

## ğŸ¨ Ã‰tape 5 : DIFFUSION (Ajout des DÃ©tails RÃ©alistes)

### ğŸ¯ But
Ajouter la "texture" et les dÃ©tails fins Ã  l'image mÃ©tÃ©o, comme des nuages, des tourbillons, des gradients subtils.

### ğŸ” Analogie du Squelette et de la Chair

Imaginez un dessinateur qui travaille en deux Ã©tapes :
1. **Le RCN dessine le squelette** : Les grandes lignes, la structure gÃ©nÃ©rale ("il y a une montagne ici, un nuage lÃ ")
2. **La diffusion ajoute la chair** : Les dÃ©tails rÃ©alistes (texture du nuage, ombres, dÃ©gradÃ©s)

### ğŸ“ Exemple Concret

**EntrÃ©e 1 - Le Conditionnement (du RCN)** :
```
Instructions du RCN : 
"Zone de basse pression au centre"
"HumiditÃ© Ã©levÃ©e"
"Vent du sud-ouest"
```

**EntrÃ©e 2 - Bruit AlÃ©atoire** :
```
Image 172Ã—179 remplie de bruit alÃ©atoire (comme la neige sur un vieux tÃ©lÃ©viseur)
```

**Processus - 1000 Ã‰tapes de "DÃ©bruitage"** :

```
Ã‰tape 0 : â–“â–“â–“â–“â–“â–“â–“â–“ (bruit pur)
Ã‰tape 100 : â–“â–“â–’â–’â–‘â–‘â–“â–“ (vagues formes)
Ã‰tape 500 : â–’â–’â–‘â–‘  â–’â–’ (structures apparaissent)
Ã‰tape 1000 : â˜ï¸ â›… ğŸŒ¤ï¸ (nuages dÃ©taillÃ©s !)
```

Ã€ chaque Ã©tape, le modÃ¨le :
1. Regarde le bruit actuel
2. Consulte les "instructions" du RCN
3. EnlÃ¨ve un peu de bruit en suivant ces instructions
4. RÃ©pÃ¨te 1000 fois

**Sortie - Le RÃ©sidu** :
```
Image haute rÃ©solution 172Ã—179 avec :
- Structures de nuages rÃ©alistes
- Gradients de tempÃ©rature subtils
- Tourbillons et fronts mÃ©tÃ©o dÃ©taillÃ©s
```

### ğŸ”§ Pourquoi la Diffusion et pas juste un CNN ?

| MÃ©thode | ProblÃ¨me | Avantage Diffusion |
|:--------|:---------|:-------------------|
| **CNN Simple** | RÃ©sultats flous, manque de dÃ©tails | GÃ©nÃ¨re des textures fines |
| **GAN** | Instable, difficile Ã  entraÃ®ner | Plus stable, convergence garantie |
| **Interpolation** | Trop lisse, pas rÃ©aliste | Capture la complexitÃ© naturelle |

### ğŸŒŸ Le RÃ´le du Conditionnement

**Sans conditionnement** : La diffusion gÃ©nÃ©rerait n'importe quels nuages (alÃ©atoires).

**Avec conditionnement** : Les nuages gÃ©nÃ©rÃ©s sont cohÃ©rents avec la mÃ©tÃ©o prÃ©dite par le RCN.

**Exemple** :
```
RCN dit : "Haute pression, temps sec"
â†’ Diffusion gÃ©nÃ¨re : Peu de nuages, ciel dÃ©gagÃ©

RCN dit : "Basse pression, humiditÃ© Ã©levÃ©e"
â†’ Diffusion gÃ©nÃ¨re : Beaucoup de nuages, structures complexes
```

### ğŸ“‰ Fonction de Perte

#### L_diff (Diffusion Loss)
**But** : Apprendre Ã  prÃ©dire le bruit qu'on a ajoutÃ© Ã  une image.

**Comment Ã§a marche ?** :
1. On prend une vraie image mÃ©tÃ©o haute rÃ©solution.
2. On lui ajoute du bruit (on connaÃ®t exactement ce bruit).
3. On demande au modÃ¨le de deviner quel bruit on a ajoutÃ©.
4. On compare sa prÃ©diction avec le vrai bruit.

```
Bruit rÃ©el ajoutÃ© : [0.5, -0.3, 0.2, ...]
Bruit prÃ©dit : [0.48, -0.31, 0.19, ...]
Erreur = MSE = 0.0012
â†’ Plus l'erreur est petite, mieux le modÃ¨le apprend !
```

### âš™ï¸ HyperparamÃ¨tres

| ParamÃ¨tre | Valeur | Ã€ quoi Ã§a sert ? |
|:----------|:-------|:-----------------|
| `num_diffusion_steps` | 1000 | Combien d'Ã©tapes de dÃ©bruitage |
| `conditioning_dim` | 128 | Taille du "message" venant du RCN |
| `in_channels` | 3 | Nombre de variables mÃ©tÃ©o Ã  gÃ©nÃ©rer (T_min, T_mean, T_max) |

---

## ğŸ“Š Ã‰tape 6 : LOSS (Fonctions de Perte) - Comment le ModÃ¨le Apprend

### ğŸ¯ Vue d'Ensemble

Le modÃ¨le apprend en minimisant 3 types d'erreurs simultanÃ©ment. C'est comme un Ã©tudiant Ã©valuÃ© sur 3 critÃ¨res diffÃ©rents.

### ğŸ” Analogie du Professeur

Imaginez un professeur qui corrige un devoir de gÃ©ographie avec 3 critÃ¨res :
1. **EsthÃ©tique** : Le dessin de la carte est-il joli et dÃ©taillÃ© ? â†’ **L_diff**
2. **Exactitude** : Les donnÃ©es correspondent-elles Ã  la rÃ©alitÃ© ? â†’ **L_rec**
3. **CohÃ©rence** : Les explications logiques sont-elles correctes ? â†’ **L_dag**

### ğŸ“ DÃ©tail des 3 Pertes

#### 1. L_diff (Loss de Diffusion) = QualitÃ© Visuelle

**Formule** : MSE (Mean Squared Error) entre le bruit prÃ©dit et le bruit rÃ©el

**Exemple chiffrÃ©** :
```
Pixel 1 : Bruit rÃ©el = 0.5, PrÃ©dit = 0.48 â†’ Erreur = (0.5-0.48)Â² = 0.0004
Pixel 2 : Bruit rÃ©el = -0.3, PrÃ©dit = -0.31 â†’ Erreur = (-0.3+0.31)Â² = 0.0001
...
Moyenne sur tous les pixels = 0.0012
```

**Poids** : Î»_gen = 1.0 (prioritÃ© normale)

#### 2. L_rec (Loss de Reconstruction) = FidÃ©litÃ© aux DonnÃ©es

**But** : VÃ©rifier que le RCN peut reconstruire les observations originales.

**Exemple chiffrÃ©** :
```
Variable originale : TempÃ©rature = [15.2, 14.8, 16.1, ...]
Reconstruite par RCN : [15.1, 14.9, 16.0, ...]
Erreur par point : [0.1Â², 0.1Â², 0.1Â², ...] 
Moyenne = 0.01
```

**Poids** : Î²_rec = 0.1 (10% de l'importance totale)

#### 3. L_dag (Loss de CausalitÃ©) = Respect des Lois Physiques

**Formule** : Trace(e^(AÂ²)) - q (Contrainte NO TEARS)

**Explication simple** :
- Le modÃ¨le apprend une matrice A qui dit "qui cause quoi".
- Cette perte punit les cycles impossibles (A cause B, B cause C, C cause A).
- Plus la valeur est proche de 0, mieux les relations causales sont respectÃ©es.

**Exemple** :
```
Matrice A_dag (3Ã—3) :
       P    V    T
P   [ 0   0.5  0.2 ]  (Pression cause Vent et TempÃ©rature)
V   [ 0    0   0.1 ]  (Vent cause TempÃ©rature)
T   [ 0    0    0  ]  (TempÃ©rature ne cause rien d'autre)

L_dag = 0.02 (proche de 0 â†’ bon !)
```

**Poids** : Î³_dag = 0.1 (10% de l'importance totale)

### ğŸ§® La Formule Totale

$$L_{total} = 1.0 \times L_{diff} + 0.1 \times L_{rec} + 0.1 \times L_{dag}$$

**Exemple de calcul** :
```
L_diff = 0.0012
L_rec = 0.01
L_dag = 0.02

L_total = (1.0 Ã— 0.0012) + (0.1 Ã— 0.01) + (0.1 Ã— 0.02)
        = 0.0012 + 0.001 + 0.002
        = 0.0042

â†’ Le modÃ¨le essaie de rÃ©duire ce nombre Ã  chaque itÃ©ration !
```

---

## ğŸ“ Ã‰tape 7 : TRAINING (Boucle d'Apprentissage)

### ğŸ¯ But
RÃ©pÃ©ter le processus d'apprentissage jusqu'Ã  ce que le modÃ¨le devienne excellent.

### ğŸ” Analogie
Comme apprendre Ã  jouer du piano : vous jouez un morceau, le professeur vous dit ce qui ne va pas, vous ajustez, et vous rejouez. RÃ©pÃ©tez 10,000 fois !

### ğŸ”„ Le Cycle d'EntraÃ®nement

```
ItÃ©ration 1 :
  1. Prendre un batch de donnÃ©es (ex: 6 jours de mÃ©tÃ©o)
  2. Forward Pass : DATA â†’ GRAPH â†’ ENCODER â†’ RCN â†’ DIFFUSION
  3. Calculer L_total (comparer prÃ©diction vs rÃ©alitÃ©)
  4. Backward Pass : Calculer les gradients (oÃ¹ amÃ©liorer ?)
  5. Mettre Ã  jour les poids du modÃ¨le
  
ItÃ©ration 2 :
  [RÃ©pÃ©ter...]
  
ItÃ©ration 10,000 :
  [ModÃ¨le de plus en plus prÃ©cis !]
```

### ğŸ“‰ Ã‰volution Typique de la Loss

```
Epoch 1 : L_total = 0.5000 (modÃ¨le trÃ¨s mauvais)
Epoch 5 : L_total = 0.0500 (commence Ã  apprendre)
Epoch 10 : L_total = 0.0080 (beaucoup mieux)
Epoch 20 : L_total = 0.0042 (bon rÃ©sultat)
```

### âš™ï¸ HyperparamÃ¨tres d'EntraÃ®nement

| ParamÃ¨tre | Valeur | Ã€ quoi Ã§a sert ? |
|:----------|:-------|:-----------------|
| `learning_rate` | 0.0001 | Vitesse d'apprentissage (trop grand = instable, trop petit = lent) |
| `epochs` | 20 | Combien de fois on parcourt toutes les donnÃ©es |
| `gradient_clipping` | 1.0 | EmpÃªche les mises Ã  jour trop brutales |
| `optimizer` | Adam | Algorithme d'optimisation utilisÃ© |

---

## ğŸ“‹ Tableau RÃ©capitulatif des HyperparamÃ¨tres

### ğŸ›ï¸ Tous les RÃ©glages en un Coup d'Å’il

| Module | ParamÃ¨tre | Valeur DÃ©faut | Explication Simple |
|:-------|:----------|:--------------|:-------------------|
| **DATA** | `seq_len` | 6 | Longueur de la sÃ©quence temporelle (jours) |
| | `normalize` | Oui | Mettre les donnÃ©es Ã  la mÃªme Ã©chelle |
| | `baseline_strategy` | "hr_smoothing" | MÃ©thode pour crÃ©er l'image floue de rÃ©fÃ©rence |
| | `baseline_factor` | 4 | Combien on lisse l'image de base |
| **GRAPH** | `lr_shape` | (23, 26) | Taille de la grille basse rÃ©solution |
| | `hr_shape` | (172, 179) | Taille de la grille haute rÃ©solution |
| | `include_mid_layer` | Non | Ajouter des niveaux atmosphÃ©riques intermÃ©diaires |
| **ENCODER** | `hidden_dim` | 128 | Taille des rÃ©sumÃ©s intelligents |
| | `conditioning_dim` | 128 | Taille du message vers la diffusion |
| | `metapaths` | 3 | Nombre de types de relations mÃ©tÃ©o |
| **RCN** | `num_vars` | 3 | Nombre de phÃ©nomÃ¨nes suivis (advection, convection, statique) |
| | `hidden_dim` | 128 | Taille de la mÃ©moire |
| | `driver_dim` | 15 | Nombre de variables d'entrÃ©e |
| | `dropout` | 0.0 | Taux d'oubli volontaire (rÃ©gularisation) |
| **DIFFUSION** | `num_diffusion_steps` | 1000 | Nombre d'Ã©tapes de dÃ©bruitage |
| | `in_channels` | 3 | Nombre de variables Ã  gÃ©nÃ©rer |
| | `conditioning_dim` | 128 | Taille du message reÃ§u du RCN |
| **LOSS** | `lambda_gen` | 1.0 | Importance de la qualitÃ© visuelle |
| | `beta_rec` | 0.1 | Importance de la fidÃ©litÃ© aux donnÃ©es |
| | `gamma_dag` | 0.1 | Importance du respect de la causalitÃ© |
| **TRAINING** | `learning_rate` | 0.0001 | Vitesse d'apprentissage |
| | `epochs` | 20 | Nombre de passages sur toutes les donnÃ©es |
| | `gradient_clipping` | 1.0 | Limite des mises Ã  jour |

---

## ğŸ¬ Conclusion : Le Voyage Complet des DonnÃ©es

RÃ©capitulons le voyage d'une donnÃ©e mÃ©tÃ©o, du dÃ©but Ã  la fin :

1. **DATA** : "Fichier NetCDF brut" â†’ "DonnÃ©es normalisÃ©es prÃªtes"
2. **GRAPH** : "Grille 23Ã—26" â†’ "RÃ©seau de 598 nÅ“uds connectÃ©s"
3. **ENCODER** : "15 variables brutes" â†’ "3 rÃ©sumÃ©s intelligents H(0)"
4. **RCN** : "H(0)" â†’ "Ã‰volution temporelle H(1), H(2)... H(6)"
5. **DIFFUSION** : "Instructions + Bruit" â†’ "Image dÃ©taillÃ©e 172Ã—179"
6. **LOSS** : "Comparer avec la rÃ©alitÃ©" â†’ "Ajuster le modÃ¨le"
7. **TRAINING** : RÃ©pÃ©ter 10,000 fois â†’ "ModÃ¨le expert !"

**RÃ©sultat Final** : Une carte mÃ©tÃ©o dÃ©taillÃ©e et rÃ©aliste, respectant la physique, avec des textures fines que les mÃ©thodes classiques ne peuvent pas gÃ©nÃ©rer !

---

## ğŸ’¡ Questions FrÃ©quentes

**Q : Pourquoi ne pas juste interpoler (agrandir) l'image basse rÃ©solution ?**
R : L'interpolation donne des rÃ©sultats flous. Elle ne peut pas inventer les dÃ©tails fins comme les structures de nuages ou les tourbillons.

**Q : Ã‡a prend combien de temps Ã  entraÃ®ner ?**
R : Avec un GPU moderne, quelques heures Ã  quelques jours selon la quantitÃ© de donnÃ©es.

**Q : Le modÃ¨le peut-il se tromper ?**
R : Oui ! C'est pour Ã§a qu'on utilise 3 fonctions de perte pour le surveiller. Plus on l'entraÃ®ne, moins il se trompe.

**Q : Pourquoi c'est si compliquÃ© ?**
R : Parce que la mÃ©tÃ©o est un systÃ¨me complexe avec de la mÃ©moire temporelle (ce qui s'est passÃ© hier compte) et des relations causales (la pression cause le vent). Un modÃ¨le simple ne peut pas capturer tout Ã§a.

---

**ğŸ“– Pour aller plus loin, consultez :**
- `RAPPORT_TECHNIQUE_COMPLET.md` : Version technique dÃ©taillÃ©e
- `ARCHITECTURE_MODEL.md` : Architecture complÃ¨te avec formules mathÃ©matiques



